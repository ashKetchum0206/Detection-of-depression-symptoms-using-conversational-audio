{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "random.seed(69)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encoding_categorical_variables(data):\n",
    "\n",
    "    all_cols = data.columns.tolist()\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['at_home', 'teacher', 'health','services','other']])\n",
    "    data[['Mjob']] = encoder.fit_transform(data[['Mjob']]).astype('int64')\n",
    "    data[['Fjob']] = encoder.fit_transform(data[['Fjob']]).astype('int64')\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['other', 'mother', 'father']])\n",
    "    data[['guardian']] = encoder.fit_transform(data[['guardian']]).astype('int64')\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['home', 'reputation', 'course', 'other']])\n",
    "    data[['reason']] = encoder.fit_transform(data[['reason']]).astype('int64')\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['no', 'yes']])\n",
    "    data[['schoolsup']] = encoder.fit_transform(data[['schoolsup']]).astype('int64')\n",
    "    data[['famsup']] = encoder.fit_transform(data[['famsup']]).astype('int64')\n",
    "    data[['paid']] = encoder.fit_transform(data[['paid']]).astype('int64')\n",
    "    data[['activities']] = encoder.fit_transform(data[['activities']]).astype('int64')\n",
    "    data[['nursery']] = encoder.fit_transform(data[['nursery']]).astype('int64')\n",
    "    data[['higher']] = encoder.fit_transform(data[['higher']]).astype('int64')\n",
    "    data[['internet']] = encoder.fit_transform(data[['internet']]).astype('int64')\n",
    "    data[['romantic']] = encoder.fit_transform(data[['romantic']]).astype('int64')\n",
    "\n",
    "    categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus']\n",
    "    categorical_data = data[categorical_cols]\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_data = encoder.fit_transform(categorical_data).astype('int64')\n",
    "\n",
    "    data_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    numeric_cols = [col for col in all_cols if col not in categorical_cols]\n",
    "\n",
    "    data = pd.concat([data_encoded, data[numeric_cols]], axis=1)\n",
    "\n",
    "    # Droping column 'age' as G3 scores are independent of 'age'\n",
    "    data.drop('age', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def target_variable_encoding(data, data_test):\n",
    "\n",
    "  # For binary classification tasks, according to the research paper, pass is considered if G3 >= 10, else fail\n",
    "    for index, row in data.iterrows():\n",
    "        if row['G3'] >= 13:\n",
    "            data.at[index, 'G3'] = 1\n",
    "        else:\n",
    "            data.at[index, 'G3'] = 0\n",
    "\n",
    "    for index, row in data_test.iterrows():\n",
    "        if row['G3'] >= 13:\n",
    "            data_test.at[index, 'G3'] = 1\n",
    "        else:\n",
    "            data_test.at[index, 'G3'] = 0\n",
    "\n",
    "    return [data, data_test]\n",
    "\n",
    "def test_train_split(data):\n",
    "\n",
    "    # Out of 649 samples, 120 random samples are considered for the test set, and remaining ones for training.\n",
    "    test_data_indices = random.sample(range(650), 120)\n",
    "\n",
    "    data_test = data.iloc[test_data_indices].copy()\n",
    "    data.drop(index=test_data_indices, inplace=True)\n",
    "\n",
    "    data_test.reset_index(drop=True, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return [data, data_test]\n",
    "\n",
    "def feature_scaling(data, data_test):\n",
    "\n",
    "  # All these columns are considered for feature scaling, to bring their value between 0 & 1\n",
    "    feature_scaling_cols = ['Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures',\n",
    "                          'famrel','freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    data[feature_scaling_cols] = scaler.fit_transform(data[feature_scaling_cols])\n",
    "\n",
    "    data_test[feature_scaling_cols] = scaler.transform(data_test[feature_scaling_cols])\n",
    "\n",
    "    return [data, data_test]\n",
    "\n",
    "def train_test_split_arrays(data, data_test):\n",
    "\n",
    "    X_train = data.iloc[:, :-1].values\n",
    "\n",
    "    # The target variable variable 'G3' is considered as 'y'\n",
    "    y_train = data.iloc[:, -1:].values\n",
    "\n",
    "\n",
    "    X_test = data_test.iloc[:, :-1].values\n",
    "    y_test = data_test.iloc[:, -1:].values\n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('student-por.csv', sep=\",\")\n",
    "data = encoding_categorical_variables(data)\n",
    "data, data_test = test_train_split(data)\n",
    "data, data_test = feature_scaling(data, data_test)\n",
    "data, data_test = target_variable_encoding(data, data_test)\n",
    "x_train, y_train, x_test, y_test = train_test_split_arrays(data, data_test)\n",
    "x = np.vstack((x_train, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairness_subset(X, predictions, indices):\n",
    "    num_0 = 0 \n",
    "    num_1 = 0 \n",
    "    sum_0 = 0 \n",
    "    sum_1 = 0 \n",
    "\n",
    "    for i in indices:\n",
    "        if(X[i][2]): \n",
    "            num_1 += 1 \n",
    "            sum_1 += predictions[i] == 1 \n",
    "\n",
    "        else:\n",
    "            num_0 += 1 \n",
    "            sum_0 += predictions[i] == 1 \n",
    "    \n",
    "    if(num_0 > 0 and num_1 > 0): return np.abs(sum_1/num_1 - sum_0/num_0)\n",
    "    elif(num_0 == 0): return np.abs(sum_1/num_1)\n",
    "    return np.abs(sum_0/num_0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_set(subset):\n",
    "    num_0 = 0 \n",
    "    sum_0 = 0 \n",
    "    num_1 = 0 \n",
    "    sum_1 = 0\n",
    "\n",
    "    num_1 = len(subset[0]) + len(subset[1]) \n",
    "    num_0 = len(subset[2]) + len(subset[3]) \n",
    "    sum_1 = len(subset[1])\n",
    "    sum_0 = len(subset[3])\n",
    "    \n",
    "    if(num_0 > 0 and num_1 > 0): return np.abs(sum_1/num_1 - sum_0/num_0)\n",
    "    elif(num_0 == 0): return np.abs(sum_1/num_1)\n",
    "    return np.abs(sum_0/num_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train.ravel())\n",
    "predictions = []\n",
    "n = x.shape[0]\n",
    "for i in range(n):\n",
    "    predictions.append(model.predict(x[i].reshape(1,-1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_female_one = set()\n",
    "X_female_zero = set()\n",
    "X_male_one = set()\n",
    "X_male_zero = set()\n",
    "\n",
    "for i in range(n):\n",
    "    if(x[i][2]):\n",
    "        if(predictions[i]): X_female_one.add(i)\n",
    "        else: X_female_zero.add(i)\n",
    "    else:\n",
    "        if(predictions[i]): X_male_one.add(i)\n",
    "        else: X_male_zero.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_S = get_fairness_subset(x, predictions, range(n))\n",
    "budget = 300\n",
    "eps = 0.2 \n",
    "best_subset = [set(), set(), set(), set()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_0 = len(X_female_one) + len(X_male_zero) \n",
    "group_0_1 = len(X_female_zero) + len(X_male_one) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the budget is less than length of dataset/2 \n",
    "done = 0 \n",
    "if(group_1_0 > group_0_1):\n",
    "    while(done < budget):\n",
    "        if(len(X_female_one) > 0):\n",
    "            new_el = random.sample(sorted(X_female_one), 1)[0]\n",
    "            X_female_one.remove(new_el)\n",
    "            best_subset[1].add(new_el) \n",
    "            done+=1\n",
    "\n",
    "        if(len(X_male_zero) > 0):\n",
    "            new_el = random.sample(sorted(X_male_zero), 1)[0]\n",
    "            X_male_zero.remove(new_el)\n",
    "            best_subset[2].add(new_el) \n",
    "            done+=1\n",
    "        # print(done)\n",
    "\n",
    "else:\n",
    "    while(done < budget):\n",
    "        if(len(X_female_zero) > 0):\n",
    "            new_el = random.sample(sorted(X_female_zero), 1)[0]\n",
    "            X_female_zero.remove(new_el)\n",
    "            best_subset[0].add(new_el) \n",
    "            done+=1\n",
    "\n",
    "        if(len(X_male_one) > 0):\n",
    "            new_el = random.sample(sorted(X_male_one), 1)[0]\n",
    "            X_male_one.remove(new_el)\n",
    "            best_subset[3].add(new_el) \n",
    "            done+=1\n",
    "        # print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_h = fairness_set(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.abs(u_h - u_S > eps):\n",
    "\n",
    "    if(group_1_0 > group_0_1):\n",
    "\n",
    "        if(len(X_female_zero) > 0):\n",
    "            new_el = random.sample(sorted(X_female_zero), 1)[0]\n",
    "            X_female_zero.remove(new_el)\n",
    "            best_subset[0].add(new_el) \n",
    "\n",
    "            remove_el = random.sample(sorted(best_subset[1]), 1)[0]\n",
    "            best_subset[1].remove(remove_el)\n",
    "            X_female_one.add(remove_el)\n",
    "\n",
    "        if(len(X_male_one) > 0):\n",
    "            new_el = random.sample(sorted(X_male_one), 1)[0]\n",
    "            X_male_one.remove(new_el)\n",
    "            best_subset[3].add(new_el) \n",
    "\n",
    "            remove_el = random.sample(sorted(best_subset[2]), 1)[0]\n",
    "            best_subset[2].remove(remove_el)\n",
    "            X_male_zero.add(remove_el)\n",
    "\n",
    "    else:\n",
    "        if(len(X_female_one) > 0):\n",
    "            new_el = random.sample(sorted(X_female_one), 1)[0]\n",
    "            X_female_one.remove(new_el)\n",
    "            best_subset[1].add(new_el) \n",
    "\n",
    "            remove_el = random.sample(sorted(best_subset[0]), 1)[0]\n",
    "            best_subset[0].remove(remove_el)\n",
    "            X_female_zero.add(remove_el)\n",
    "\n",
    "        if(len(X_male_zero) > 0):\n",
    "            new_el = random.sample(sorted(X_male_zero), 1)[0]\n",
    "            X_male_zero.remove(new_el)\n",
    "            best_subset[2].add(new_el) \n",
    "\n",
    "            remove_el = random.sample(sorted(best_subset[3]), 1)[0]\n",
    "            best_subset[3].remove(remove_el)\n",
    "            X_male_one.add(remove_el)\n",
    "\n",
    "    u_h = fairness_set(best_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39999999999999997, 0.2060896366241976)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_h, u_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = 0 \n",
    "best_subset = [set(), set(), set(), set()]\n",
    "\n",
    "for i in range(min(len(X_female_one), budget - done)):\n",
    "    new_el = random.sample(sorted(X_female_one), 1)[0]\n",
    "    X_female_one.remove(new_el)\n",
    "    best_subset[1].add(new_el) \n",
    "    done+=1 \n",
    "\n",
    "for i in range(min(len(X_male_zero), budget - done)):\n",
    "    new_el = random.sample(sorted(X_male_zero), 1)[0]\n",
    "    X_male_zero.remove(new_el)\n",
    "    best_subset[2].add(new_el) \n",
    "    done+=1  \n",
    "\n",
    "for i in range(min(len(X_female_zero), budget - done)):\n",
    "    new_el = random.sample(sorted(X_female_zero), 1)[0]\n",
    "    X_female_zero.remove(new_el)\n",
    "    best_subset[0].add(new_el) \n",
    "    done+=1\n",
    "\n",
    "for i in range(min(len(X_male_one), budget - done)):\n",
    "    new_el = random.sample(sorted(X_male_one), 1)[0]\n",
    "    X_male_one.remove(new_el)\n",
    "    best_subset[3].add(new_el) \n",
    "    done+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "u_h = fairness_set(best_subset)\n",
    "print(u_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.98\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.91\n",
      "0.9\n",
      "0.89\n",
      "0.88\n",
      "0.87\n",
      "0.86\n",
      "0.85\n",
      "0.84\n",
      "0.83\n",
      "0.82\n",
      "0.81\n",
      "0.8\n",
      "0.79\n",
      "0.78\n",
      "0.77\n",
      "0.76\n",
      "0.75\n",
      "0.74\n",
      "0.73\n",
      "0.72\n",
      "0.71\n",
      "0.7\n",
      "0.69\n",
      "0.68\n",
      "0.67\n",
      "0.66\n",
      "0.65\n",
      "0.64\n",
      "0.63\n",
      "0.62\n",
      "0.61\n",
      "0.6\n",
      "0.59\n",
      "0.58\n",
      "0.57\n",
      "0.56\n",
      "0.55\n",
      "0.54\n",
      "0.53\n",
      "0.52\n",
      "0.51\n",
      "0.5\n",
      "0.49\n",
      "0.48\n",
      "0.47\n",
      "0.46\n",
      "0.45\n",
      "0.44\n",
      "0.43\n",
      "0.42\n",
      "0.41\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "while np.abs(u_h - u_S > eps):\n",
    "    if(len(X_female_zero) > 0): \n",
    "        remove_el = random.sample(sorted(best_subset[1]), 1)[0] \n",
    "        best_subset[1].remove(remove_el)\n",
    "        X_female_one.add(remove_el)\n",
    "\n",
    "        new_el = random.sample(sorted(X_female_zero), 1)[0]\n",
    "        X_female_zero.remove(new_el)\n",
    "        best_subset[0].add(new_el)\n",
    "\n",
    "    elif(len(X_male_zero) > 0):\n",
    "        remove_el = random.sample(sorted(best_subset[1]), 1)[0] \n",
    "        best_subset[1].remove(remove_el)\n",
    "        X_female_one.add(remove_el)\n",
    "\n",
    "        new_el = random.sample(sorted(X_male_zero), 1)[0]\n",
    "        X_male_zero.remove(new_el)\n",
    "        best_subset[2].add(new_el)\n",
    "\n",
    "    elif(len(X_male_one) > 0):\n",
    "        remove_el = random.sample(sorted(best_subset[1]), 1)[0] \n",
    "        best_subset[1].remove(remove_el)\n",
    "        X_female_one.add(remove_el)\n",
    "\n",
    "        new_el = random.sample(sorted(X_male_one), 1)[0]\n",
    "        X_male_one.remove(new_el)\n",
    "        best_subset[3].add(new_el)\n",
    "\n",
    "    u_h = fairness_set(best_subset)\n",
    "    print(u_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the company's objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(best_subset):\n",
    "    return len(best_subset[0]), len(best_subset[1]), len(best_subset[2]), len(best_subset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.tanh(F.relu(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_util(X, model, indices):\n",
    "    num_0 = 0 \n",
    "    num_1 = 0 \n",
    "    sum_0 = 0 \n",
    "    sum_1 = 0 \n",
    "    for i in indices:\n",
    "        \n",
    "        if(X[i][2]): \n",
    "            num_1 += 1 \n",
    "            sum_1 += model(tensor(X[i], dtype=torch.float))\n",
    "\n",
    "        else:\n",
    "            num_0 += 1 \n",
    "            sum_0 +=  model(tensor(X[i], dtype=torch.float))\n",
    "\n",
    "    return torch.abs(sum_1/num_1 - sum_0/num_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = x.shape[1]\n",
    "logistic_model = logistic(num_features)\n",
    "opt = torch.optim.Adam(logistic_model.parameters(), lr = 1e-3)\n",
    "l = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_params_weights = tensor(model.coef_, dtype = torch.float)\n",
    "# pivot_params_bias = tensor(model.intercept_, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_model.layer1.weight = nn.parameter.Parameter(pivot_params_weights)\n",
    "# logistic_model.layer1.bias = nn.parameter.Parameter(pivot_params_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 1000\n",
    "# dataloader = DataLoader(list(zip(x_train,y_train)), batch_size = 32, shuffle = True)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     total_num = 0 \n",
    "#     num_classes = 2 \n",
    "#     for data,label in dataloader: \n",
    "\n",
    "#         preds = logistic_model(tensor(data, dtype = torch.float))\n",
    "#         one_hot_labels = F.one_hot(label, num_classes=num_classes).squeeze()\n",
    "#         loss = F.binary_cross_entropy_with_logits(preds, tensor(one_hot_labels, dtype = torch.float))\n",
    "#         total_loss += loss.item()\n",
    "#         total_num += 1\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "\n",
    "#     print(f\"The epoch is {epoch+1} and the average loss is {total_loss/total_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(linear_layer, pivot_params_weights, pivot_params_bias, l_norm):\n",
    "\n",
    "    # weights = linear_layer.weight \n",
    "    # bias = linear_layer.bias \n",
    "    \n",
    "    # new_weights = torch.clamp(weights, pivot_params_weights - l_norm, pivot_params_weights + l_norm)\n",
    "    # new_bias = torch.clamp(bias, pivot_params_bias - l_norm, pivot_params_bias + l_norm)\n",
    "\n",
    "    # linear_layer.weight = nn.parameter.Parameter(new_weights) \n",
    "    # linear_layer.bias = nn.parameter.Parameter(new_bias) \n",
    "    linear_layer.weight.data.clamp_(min=pivot_params_weights - l_norm, max=pivot_params_weights + l_norm)\n",
    "    linear_layer.bias.data.clamp_(min=pivot_params_bias - l_norm, max=pivot_params_bias + l_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10\n",
    "for trial in range(trials):  \n",
    "\n",
    "    with open('output.txt', 'a') as file:\n",
    "        file.write(f\"Trial {trial+1}\\n\")  \n",
    "    logistic_model = logistic(num_features)\n",
    "    opt = torch.optim.Adam(logistic_model.parameters(), lr = 1e-3)\n",
    "    \n",
    "    iterations = 5000\n",
    "    logistic_model.train()\n",
    "    for i in range(iterations):\n",
    "        lag_term = 0\n",
    "        for subset in best_subset:\n",
    "            for s in subset:\n",
    "                lag_term += torch.abs(logistic_model(tensor(x[s], dtype=torch.float)) - predictions[s])\n",
    "        util = lag_term\n",
    "        loss = -util \n",
    "        # loss = -lag_term\n",
    "        # loss = -smooth_util(x, logistic_model, range(n))\n",
    "        if(i % 500 == 0):\n",
    "            with open('output.txt', 'a') as file:\n",
    "                    file.write((str(lag_term.item())) + '\\n')\n",
    "        loss.backward()\n",
    "        opt.step() \n",
    "\n",
    "    weights = logistic_model.layer1.weight.detach().clone()  # Shape: (out_features, in_features)\n",
    "    bias = logistic_model.layer1.bias.detach().clone()     # Shape: (out_features,)\n",
    "\n",
    "    # Combine the parameters into a single tensor\n",
    "    pivot_params_weights = weights.flatten() \n",
    "    pivot_params_bias = bias\n",
    "\n",
    "    with open('output.txt', 'a') as file:\n",
    "        file.write('\\n')\n",
    "\n",
    "    iterations = 5000\n",
    "    for i in range(iterations):\n",
    "        fairness = smooth_util(x, logistic_model, range(n))\n",
    "        util = fairness\n",
    "        loss = -util \n",
    "        # loss = -lag_term\n",
    "        # loss = -smooth_util(x, logistic_model, range(n))\n",
    "        if(i % 500 == 0):\n",
    "            with open('output.txt', 'a') as file:\n",
    "                    file.write((str(fairness.item())) + '\\n')\n",
    "        loss.backward()\n",
    "        opt.step() \n",
    "        project(logistic_model.layer1, pivot_params_weights, pivot_params_bias, 5)\n",
    "\n",
    "    lag_term = 0\n",
    "    for subset in best_subset:\n",
    "        for s in subset:\n",
    "            lag_term += torch.abs(logistic_model(tensor(x[s], dtype=torch.float)) - predictions[s])\n",
    "\n",
    "    with open('output.txt', 'a') as file:\n",
    "        file.write(\"Lagrangian term: \" + str(lag_term.item()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
